{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Synthesis Research Pipeline - Experimentation Notebook\n",
    "\n",
    "This notebook provides a comprehensive research environment for experimenting with text-to-speech synthesis models. It demonstrates the key components of the TTS pipeline and allows for interactive experimentation.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The pipeline includes:\n",
    "- Text preprocessing and phoneme conversion\n",
    "- Neural TTS model (Tacotron-style architecture)\n",
    "- Audio processing and mel spectrogram generation\n",
    "- Model training and evaluation\n",
    "- Speech synthesis and quality assessment\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "# Core libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TTS Pipeline components\n",
    "from models.tacotron import TacotronTTS\n",
    "from models.vocoder import SimpleVocoder, MelGAN\n",
    "from preprocessing.text_processor import TextProcessor\n",
    "from preprocessing.audio_processor import AudioProcessor\n",
    "from training.trainer import TTSTrainer\n",
    "from training.dataset import TTSDataset, create_data_loader\n",
    "from synthesis.synthesizer import TTSSynthesizer\n",
    "from evaluation.metrics import AudioMetrics, TTSMetrics\n",
    "from utils.config import load_config, get_default_config\n",
    "from utils.visualization import plot_spectrogram, plot_attention_weights, plot_training_metrics\n",
    "from utils.audio_utils import save_audio, load_audio\n",
    "from utils.logger import setup_logger\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Set up logging\n",
    "logger = setup_logger('experiment', level='INFO')\n",
    "\n",
    "print(\"âœ… All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration and Setup\n",
    "\n",
    "First, let's load the configuration and set up our experimental environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config('../config/model_config.yaml')\n",
    "\n",
    "# Override some parameters for experimentation\n",
    "config['training']['batch_size'] = 8  # Smaller batch for notebook\n",
    "config['training']['num_epochs'] = 5  # Fewer epochs for quick testing\n",
    "config['dataset']['max_text_length'] = 100\n",
    "config['dataset']['max_mel_length'] = 500\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"Model embedding dim: {config['model']['embedding_dim']}\")\n",
    "print(f\"Audio sample rate: {config['audio']['sample_rate']}\")\n",
    "print(f\"Training batch size: {config['training']['batch_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Processing Experiments\n",
    "\n",
    "Let's explore the text preprocessing pipeline and see how different texts are processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text processor\n",
    "text_processor = TextProcessor()\n",
    "\n",
    "# Test texts for experimentation\n",
    "test_texts = [\n",
    "    \"Hello, world! This is a test of the speech synthesis system.\",\n",
    "    \"Dr. Smith visited the U.S.A. on January 1st, 2024.\",\n",
    "    \"The quick brown fox jumps over 123 lazy dogs!!!\",\n",
    "    \"Speech synthesis, or text-to-speech (TTS), is the artificial production of human speech.\"\n",
    "]\n",
    "\n",
    "print(\"Text Processing Experiments\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, text in enumerate(test_texts):\n",
    "    print(f\"\\nTest {i+1}: {text}\")\n",
    "    \n",
    "    # Apply preprocessing steps\n",
    "    normalized = text_processor.normalize_text(text)\n",
    "    expanded = text_processor.expand_abbreviations(normalized)\n",
    "    numbers_converted = text_processor.convert_numbers(expanded)\n",
    "    cleaned = text_processor.clean_text(numbers_converted)\n",
    "    \n",
    "    print(f\"  Normalized: {normalized}\")\n",
    "    print(f\"  Expanded: {expanded}\")\n",
    "    print(f\"  Numbers: {numbers_converted}\")\n",
    "    print(f\"  Cleaned: {cleaned}\")\n",
    "    \n",
    "    # Convert to sequence\n",
    "    sequence = text_processor.text_to_sequence(cleaned)\n",
    "    print(f\"  Sequence length: {len(sequence)}\")\n",
    "    \n",
    "    # Try phoneme conversion\n",
    "    try:\n",
    "        phonemes = text_processor.text_to_phonemes(cleaned)\n",
    "        print(f\"  Phonemes: {' '.join(phonemes[:10])}{'...' if len(phonemes) > 10 else ''}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Phonemes: Error - {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze vocabulary and character distributions\n",
    "print(f\"Vocabulary size: {text_processor.vocab_size}\")\n",
    "print(f\"Character to index mapping (first 20):\")\n",
    "for i, (char, idx) in enumerate(list(text_processor.char_to_idx.items())[:20]):\n",
    "    print(f\"  '{char}': {idx}\")\n",
    "\n",
    "# Analyze sequence lengths\n",
    "sequence_lengths = []\n",
    "for text in test_texts:\n",
    "    processed = text_processor.normalize_text(text)\n",
    "    processed = text_processor.clean_text(processed)\n",
    "    sequence = text_processor.text_to_sequence(processed)\n",
    "    sequence_lengths.append(len(sequence))\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(len(sequence_lengths)), sequence_lengths)\n",
    "plt.title('Sequence Lengths')\n",
    "plt.xlabel('Test Text')\n",
    "plt.ylabel('Sequence Length')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "text_lengths = [len(text) for text in test_texts]\n",
    "plt.scatter(text_lengths, sequence_lengths)\n",
    "plt.title('Text Length vs Sequence Length')\n",
    "plt.xlabel('Original Text Length')\n",
    "plt.ylabel('Processed Sequence Length')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Audio Processing Experiments\n",
    "\n",
    "Let's explore audio processing capabilities and create synthetic audio data for experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize audio processor\n",
    "audio_processor = AudioProcessor(\n",
    "    sample_rate=config['audio']['sample_rate'],\n",
    "    n_fft=config['audio']['n_fft'],\n",
    "    hop_length=config['audio']['hop_length'],\n",
    "    n_mels=config['audio']['n_mels']\n",
    ")\n",
    "\n",
    "print(f\"Audio processor initialized:\")\n",
    "print(f\"  Sample rate: {audio_processor.sample_rate} Hz\")\n",
    "print(f\"  FFT size: {audio_processor.n_fft}\")\n",
    "print(f\"  Hop length: {audio_processor.hop_length}\")\n",
    "print(f\"  Mel channels: {audio_processor.n_mels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic audio for demonstration\n",
    "def create_synthetic_audio(duration=2.0, frequency=440, sample_rate=22050):\n",
    "    \"\"\"Create a synthetic audio signal\"\"\"\n",
    "    t = np.linspace(0, duration, int(duration * sample_rate), False)\n",
    "    \n",
    "    # Create a more complex waveform\n",
    "    audio = (np.sin(2 * np.pi * frequency * t) * 0.3 +\n",
    "             np.sin(2 * np.pi * frequency * 2 * t) * 0.2 +\n",
    "             np.sin(2 * np.pi * frequency * 3 * t) * 0.1)\n",
    "    \n",
    "    # Add some envelope\n",
    "    envelope = np.exp(-t * 0.5) * np.sin(2 * np.pi * 2 * t) * 0.5 + 0.5\n",
    "    audio *= envelope\n",
    "    \n",
    "    return audio\n",
    "\n",
    "# Generate test audio\n",
    "test_audio = create_synthetic_audio(duration=3.0, frequency=220)\n",
    "\n",
    "print(f\"Generated synthetic audio:\")\n",
    "print(f\"  Duration: {len(test_audio) / audio_processor.sample_rate:.2f} seconds\")\n",
    "print(f\"  Samples: {len(test_audio)}\")\n",
    "print(f\"  RMS: {np.sqrt(np.mean(test_audio**2)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze audio and compute mel spectrogram\n",
    "mel_spec = audio_processor.compute_mel_spectrogram(test_audio)\n",
    "\n",
    "print(f\"Mel spectrogram shape: {mel_spec.shape}\")\n",
    "print(f\"  Mel channels: {mel_spec.shape[0]}\")\n",
    "print(f\"  Time frames: {mel_spec.shape[1]}\")\n",
    "print(f\"  Time resolution: {mel_spec.shape[1] * audio_processor.hop_length / audio_processor.sample_rate:.2f} seconds\")\n",
    "\n",
    "# Visualize waveform and spectrogram\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Waveform\n",
    "plt.subplot(2, 2, 1)\n",
    "time_axis = np.linspace(0, len(test_audio) / audio_processor.sample_rate, len(test_audio))\n",
    "plt.plot(time_axis, test_audio)\n",
    "plt.title('Synthetic Audio Waveform')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "# Mel spectrogram\n",
    "plt.subplot(2, 2, 2)\n",
    "librosa.display.specshow(mel_spec, sr=audio_processor.sample_rate, \n",
    "                        hop_length=audio_processor.hop_length, \n",
    "                        x_axis='time', y_axis='mel')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel Spectrogram')\n",
    "\n",
    "# Audio features\n",
    "features = audio_processor.extract_audio_features(test_audio)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "feature_names = ['rms', 'zero_crossing_rate', 'spectral_centroid', 'spectral_rolloff']\n",
    "feature_values = [features[name] for name in feature_names]\n",
    "plt.bar(feature_names, feature_values)\n",
    "plt.title('Audio Features')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# MFCC visualization\n",
    "plt.subplot(2, 2, 4)\n",
    "mfcc_mean = features['mfcc_mean']\n",
    "plt.plot(mfcc_mean, 'o-')\n",
    "plt.title('MFCC Coefficients (Mean)')\n",
    "plt.xlabel('MFCC Index')\n",
    "plt.ylabel('Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display features\n",
    "print(\"\\nExtracted Audio Features:\")\n",
    "for key, value in features.items():\n",
    "    if isinstance(value, np.ndarray):\n",
    "        print(f\"  {key}: {value.shape} array\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture Exploration\n",
    "\n",
    "Let's examine the TTS model architecture and understand its components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TTS model\n",
    "model_config = {\n",
    "    'vocab_size': text_processor.vocab_size,\n",
    "    'embedding_dim': config['model']['embedding_dim'],\n",
    "    'encoder_dim': config['model']['encoder_dim'],\n",
    "    'decoder_dim': config['model']['decoder_dim'],\n",
    "    'attention_dim': config['model']['attention_dim'],\n",
    "    'num_mels': config['model']['num_mels']\n",
    "}\n",
    "\n",
    "model = TacotronTTS(model_config)\n",
    "\n",
    "# Model statistics\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model Architecture: {type(model).__name__}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size (approximate): {total_params * 4 / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "print(\"\\nModel Components:\")\n",
    "for name, module in model.named_children():\n",
    "    num_params = sum(p.numel() for p in module.parameters())\n",
    "    print(f\"  {name}: {num_params:,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model forward pass\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Create dummy input\n",
    "batch_size = 2\n",
    "text_length = 50\n",
    "mel_length = 100\n",
    "\n",
    "dummy_text = torch.randint(0, text_processor.vocab_size, (batch_size, text_length)).to(device)\n",
    "dummy_mel = torch.randn(batch_size, config['model']['num_mels'], mel_length).to(device)\n",
    "\n",
    "print(f\"Input shapes:\")\n",
    "print(f\"  Text: {dummy_text.shape}\")\n",
    "print(f\"  Mel: {dummy_mel.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(dummy_text, dummy_mel)\n",
    "\n",
    "print(f\"\\nOutput shapes:\")\n",
    "for key, value in outputs.items():\n",
    "    print(f\"  {key}: {value.shape}\")\n",
    "\n",
    "# Test inference mode\n",
    "with torch.no_grad():\n",
    "    inference_outputs = model.inference(dummy_text, max_len=150)\n",
    "\n",
    "print(f\"\\nInference output shapes:\")\n",
    "for key, value in inference_outputs.items():\n",
    "    print(f\"  {key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Mechanism Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention weights from inference\n",
    "attention_weights = inference_outputs['attention_weights'][0].cpu().numpy()  # First batch item\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(attention_weights.T, aspect='auto', origin='lower', cmap='Blues')\n",
    "plt.colorbar(label='Attention Weight')\n",
    "plt.title('Attention Alignment (Random Input)')\n",
    "plt.xlabel('Decoder Steps (Time)')\n",
    "plt.ylabel('Encoder Steps (Text)')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"Max attention weight: {attention_weights.max():.4f}\")\n",
    "print(f\"Min attention weight: {attention_weights.min():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset and Training Experiments\n",
    "\n",
    "Let's create a synthetic dataset and experiment with the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic dataset for experimentation\n",
    "dataset = TTSDataset(\n",
    "    data_dir='../data',  # This will use synthetic data since directory doesn't exist\n",
    "    text_processor=text_processor,\n",
    "    audio_processor=audio_processor,\n",
    "    max_text_length=config['dataset']['max_text_length'],\n",
    "    max_mel_length=config['dataset']['max_mel_length']\n",
    ")\n",
    "\n",
    "print(f\"Dataset created with {len(dataset)} samples\")\n",
    "\n",
    "# Examine a sample\n",
    "sample = dataset[0]\n",
    "print(f\"\\nSample structure:\")\n",
    "for key, value in sample.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"  {key}: {value.shape} ({value.dtype})\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader\n",
    "train_loader = create_data_loader(\n",
    "    dataset, \n",
    "    batch_size=config['training']['batch_size'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Data loader created with batch size {config['training']['batch_size']}\")\n",
    "print(f\"Number of batches: {len(train_loader)}\")\n",
    "\n",
    "# Examine a batch\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"\\nBatch structure:\")\n",
    "for key, value in batch.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"  {key}: {value.shape} ({value.dtype})\")\n",
    "    else:\n",
    "        print(f\"  {key}: {len(value)} items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = TTSTrainer(model, config=config['training'], device=device)\n",
    "\n",
    "print(f\"Trainer initialized:\")\n",
    "print(f\"  Learning rate: {trainer.learning_rate}\")\n",
    "print(f\"  Device: {trainer.device}\")\n",
    "print(f\"  Optimizer: {type(trainer.optimizer).__name__}\")\n",
    "\n",
    "# Simulate a few training steps\n",
    "model.train()\n",
    "training_losses = []\n",
    "\n",
    "print(\"\\nRunning training simulation...\")\n",
    "for step, batch in enumerate(train_loader):\n",
    "    if step >= 3:  # Only run a few steps for demonstration\n",
    "        break\n",
    "    \n",
    "    # Move batch to device\n",
    "    text_inputs = batch['text'].to(device)\n",
    "    mel_targets = batch['mel'].to(device)\n",
    "    stop_targets = batch['stop_tokens'].to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(text_inputs, mel_targets)\n",
    "    \n",
    "    # Calculate loss\n",
    "    targets = {\n",
    "        'mel_targets': mel_targets,\n",
    "        'stop_targets': stop_targets\n",
    "    }\n",
    "    losses = model.calculate_loss(outputs, targets)\n",
    "    \n",
    "    training_losses.append(losses['total_loss'].item())\n",
    "    \n",
    "    print(f\"  Step {step + 1}: Loss = {losses['total_loss'].item():.4f}\")\n",
    "    print(f\"    Mel loss: {losses['mel_loss'].item():.4f}\")\n",
    "    print(f\"    PostNet loss: {losses['mel_postnet_loss'].item():.4f}\")\n",
    "    print(f\"    Stop loss: {losses['stop_loss'].item():.4f}\")\n",
    "\n",
    "# Plot training losses\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(training_losses, 'o-')\n",
    "plt.title('Training Loss (First Few Steps)')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Speech Synthesis Experiments\n",
    "\n",
    "Let's experiment with speech synthesis using our trained (or randomly initialized) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize synthesizer\n",
    "vocoder = SimpleVocoder(mel_channels=config['model']['num_mels'])\n",
    "synthesizer = TTSSynthesizer(\n",
    "    tts_model=model,\n",
    "    vocoder=vocoder,\n",
    "    text_processor=text_processor,\n",
    "    audio_processor=audio_processor\n",
    ")\n",
    "\n",
    "print(f\"Synthesizer initialized with {type(vocoder).__name__} vocoder\")\n",
    "\n",
    "# Test synthesis\n",
    "test_text = \"Hello, this is a test of the speech synthesis system.\"\n",
    "print(f\"\\nSynthesizing: '{test_text}'\")\n",
    "\n",
    "# Get synthesis info first\n",
    "synthesis_info = synthesizer.get_synthesis_info(test_text)\n",
    "print(f\"\\nSynthesis Information:\")\n",
    "for key, value in synthesis_info.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform synthesis\n",
    "try:\n",
    "    audio, sample_rate, mel_spectrogram = synthesizer.synthesize(\n",
    "        test_text,\n",
    "        speed=1.0,\n",
    "        pitch=0.0,\n",
    "        energy=1.0,\n",
    "        max_decoder_steps=200\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nSynthesis successful!\")\n",
    "    print(f\"  Audio shape: {audio.shape}\")\n",
    "    print(f\"  Sample rate: {sample_rate} Hz\")\n",
    "    print(f\"  Duration: {len(audio) / sample_rate:.2f} seconds\")\n",
    "    print(f\"  Mel spectrogram shape: {mel_spectrogram.shape}\")\n",
    "    \n",
    "    # Visualize synthesis results\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Generated audio waveform\n",
    "    time_axis = np.linspace(0, len(audio) / sample_rate, len(audio))\n",
    "    axes[0, 0].plot(time_axis, audio)\n",
    "    axes[0, 0].set_title('Generated Audio Waveform')\n",
    "    axes[0, 0].set_xlabel('Time (s)')\n",
    "    axes[0, 0].set_ylabel('Amplitude')\n",
    "    \n",
    "    # Generated mel spectrogram\n",
    "    im = axes[0, 1].imshow(mel_spectrogram, aspect='auto', origin='lower', cmap='viridis')\n",
    "    axes[0, 1].set_title('Generated Mel Spectrogram')\n",
    "    axes[0, 1].set_xlabel('Time Frames')\n",
    "    axes[0, 1].set_ylabel('Mel Channels')\n",
    "    plt.colorbar(im, ax=axes[0, 1])\n",
    "    \n",
    "    # Audio statistics\n",
    "    audio_stats = {\n",
    "        'RMS': np.sqrt(np.mean(audio**2)),\n",
    "        'Max': np.max(np.abs(audio)),\n",
    "        'ZCR': np.mean(librosa.feature.zero_crossing_rate(audio)),\n",
    "        'Spectral Centroid': np.mean(librosa.feature.spectral_centroid(y=audio, sr=sample_rate))\n",
    "    }\n",
    "    \n",
    "    axes[1, 0].bar(audio_stats.keys(), audio_stats.values())\n",
    "    axes[1, 0].set_title('Audio Statistics')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Mel spectrogram statistics\n",
    "    mel_stats = {\n",
    "        'Mean': np.mean(mel_spectrogram),\n",
    "        'Std': np.std(mel_spectrogram),\n",
    "        'Min': np.min(mel_spectrogram),\n",
    "        'Max': np.max(mel_spectrogram)\n",
    "    }\n",
    "    \n",
    "    axes[1, 1].bar(mel_stats.keys(), mel_stats.values())\n",
    "    axes[1, 1].set_title('Mel Spectrogram Statistics')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Synthesis failed: {str(e)}\")\n",
    "    print(\"This is expected with an untrained model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prosody Control Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different prosody parameters\n",
    "prosody_params = [\n",
    "    {'speed': 0.8, 'pitch': -2, 'energy': 0.8, 'description': 'Slow, low pitch, quiet'},\n",
    "    {'speed': 1.0, 'pitch': 0, 'energy': 1.0, 'description': 'Normal'},\n",
    "    {'speed': 1.2, 'pitch': 2, 'energy': 1.2, 'description': 'Fast, high pitch, loud'}\n",
    "]\n",
    "\n",
    "synthesis_results = []\n",
    "\n",
    "for params in prosody_params:\n",
    "    try:\n",
    "        audio, sr, mel = synthesizer.synthesize(\n",
    "            \"This is a prosody test.\",\n",
    "            speed=params['speed'],\n",
    "            pitch=params['pitch'],\n",
    "            energy=params['energy'],\n",
    "            max_decoder_steps=100\n",
    "        )\n",
    "        \n",
    "        synthesis_results.append({\n",
    "            'description': params['description'],\n",
    "            'audio': audio,\n",
    "            'duration': len(audio) / sr,\n",
    "            'rms': np.sqrt(np.mean(audio**2))\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed synthesis for {params['description']}: {str(e)}\")\n",
    "\n",
    "if synthesis_results:\n",
    "    # Compare prosody results\n",
    "    descriptions = [r['description'] for r in synthesis_results]\n",
    "    durations = [r['duration'] for r in synthesis_results]\n",
    "    rms_values = [r['rms'] for r in synthesis_results]\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(descriptions, durations)\n",
    "    plt.title('Synthesis Duration by Prosody')\n",
    "    plt.ylabel('Duration (s)')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(descriptions, rms_values)\n",
    "    plt.title('Audio RMS by Prosody')\n",
    "    plt.ylabel('RMS')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\nelse:\n",
    "    print(\"No successful syntheses to compare.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation and Metrics\n",
    "\n",
    "Let's explore the evaluation capabilities of our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluation metrics\n",
    "audio_metrics = AudioMetrics(sample_rate=audio_processor.sample_rate)\n",
    "tts_metrics = TTSMetrics(sample_rate=audio_processor.sample_rate)\n",
    "\n",
    "print(\"Evaluation metrics initialized\")\n",
    "\n",
    "# Evaluate synthetic audio\n",
    "if 'audio' in locals():  # If we have synthesized audio\n",
    "    evaluation_results = audio_metrics.evaluate_audio(audio)\n",
    "    \n",
    "    print(\"\\nAudio Quality Evaluation:\")\n",
    "    for metric, value in evaluation_results.items():\n",
    "        if isinstance(value, np.ndarray):\n",
    "            print(f\"  {metric}: array shape {value.shape}\")\n",
    "        else:\n",
    "            print(f\"  {metric}: {value:.4f}\")\nelse:\n",
    "    print(\"No synthesized audio available for evaluation\")\n",
    "    # Use the synthetic audio we created earlier\n",
    "    evaluation_results = audio_metrics.evaluate_audio(test_audio)\n",
    "    \n",
    "    print(\"\\nEvaluating synthetic test audio:\")\n",
    "    for metric, value in evaluation_results.items():\n",
    "        if isinstance(value, np.ndarray):\n",
    "            print(f\"  {metric}: array shape {value.shape}\")\n",
    "        else:\n",
    "            print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate mel spectrogram quality\n",
    "if 'mel_spectrogram' in locals():\n",
    "    # Create a reference mel spectrogram (using ground truth or another synthesis)\n",
    "    reference_mel = audio_processor.compute_mel_spectrogram(test_audio)\n",
    "    \n",
    "    # Ensure shapes match for comparison\n",
    "    min_frames = min(mel_spectrogram.shape[1], reference_mel.shape[1])\n",
    "    pred_mel = mel_spectrogram[:, :min_frames]\n",
    "    ref_mel = reference_mel[:, :min_frames]\n",
    "    \n",
    "    mel_evaluation = tts_metrics.evaluate_mel_spectrogram(pred_mel, ref_mel)\n",
    "    \n",
    "    print(\"\\nMel Spectrogram Evaluation:\")\n",
    "    for metric, value in mel_evaluation.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    # Visualize comparison\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.imshow(ref_mel, aspect='auto', origin='lower', cmap='viridis')\n",
    "    plt.title('Reference Mel Spectrogram')\n",
    "    plt.ylabel('Mel Channels')\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.imshow(pred_mel, aspect='auto', origin='lower', cmap='viridis')\n",
    "    plt.title('Generated Mel Spectrogram')\n",
    "    plt.ylabel('Mel Channels')\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    difference = np.abs(ref_mel - pred_mel)\n",
    "    plt.imshow(difference, aspect='auto', origin='lower', cmap='Reds')\n",
    "    plt.title('Absolute Difference')\n",
    "    plt.xlabel('Time Frames')\n",
    "    plt.ylabel('Mel Channels')\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    ref_mean = np.mean(ref_mel, axis=1)\n",
    "    pred_mean = np.mean(pred_mel, axis=1)\n",
    "    plt.plot(ref_mean, label='Reference', linewidth=2)\n",
    "    plt.plot(pred_mean, label='Generated', linewidth=2)\n",
    "    plt.title('Mean Mel Values by Channel')\n",
    "    plt.xlabel('Mel Channel')\n",
    "    plt.ylabel('Mean Value')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\nelse:\n",
    "    print(\"No generated mel spectrogram available for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Hyperparameter Exploration\n",
    "\n",
    "Let's experiment with different model configurations and see their impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different model configurations to test\n",
    "model_configs = [\n",
    "    {\n",
    "        'name': 'Small Model',\n",
    "        'embedding_dim': 256,\n",
    "        'encoder_dim': 256,\n",
    "        'decoder_dim': 512,\n",
    "        'attention_dim': 64\n",
    "    },\n",
    "    {\n",
    "        'name': 'Medium Model', \n",
    "        'embedding_dim': 512,\n",
    "        'encoder_dim': 512,\n",
    "        'decoder_dim': 1024,\n",
    "        'attention_dim': 128\n",
    "    },\n",
    "    {\n",
    "        'name': 'Large Model',\n",
    "        'embedding_dim': 768,\n",
    "        'encoder_dim': 768,\n",
    "        'decoder_dim': 1536,\n",
    "        'attention_dim': 192\n",
    "    }\n",
    "]\n",
    "\n",
    "model_comparison = []\n",
    "\n",
    "for config_variant in model_configs:\n",
    "    # Create model config\n",
    "    test_config = {\n",
    "        'vocab_size': text_processor.vocab_size,\n",
    "        'embedding_dim': config_variant['embedding_dim'],\n",
    "        'encoder_dim': config_variant['encoder_dim'],\n",
    "        'decoder_dim': config_variant['decoder_dim'],\n",
    "        'attention_dim': config_variant['attention_dim'],\n",
    "        'num_mels': config['model']['num_mels']\n",
    "    }\n",
    "    \n",
    "    # Create and analyze model\n",
    "    test_model = TacotronTTS(test_config)\n",
    "    total_params = sum(p.numel() for p in test_model.parameters())\n",
    "    model_size_mb = total_params * 4 / 1024 / 1024\n",
    "    \n",
    "    model_comparison.append({\n",
    "        'name': config_variant['name'],\n",
    "        'parameters': total_params,\n",
    "        'size_mb': model_size_mb,\n",
    "        'embedding_dim': config_variant['embedding_dim'],\n",
    "        'encoder_dim': config_variant['encoder_dim'],\n",
    "        'decoder_dim': config_variant['decoder_dim']\n",
    "    })\n",
    "    \n",
    "    print(f\"{config_variant['name']}:\")\n",
    "    print(f\"  Parameters: {total_params:,}\")\n",
    "    print(f\"  Size: {model_size_mb:.2f} MB\")\n",
    "    print()\n",
    "\n",
    "# Visualize model comparison\n",
    "names = [m['name'] for m in model_comparison]\n",
    "params = [m['parameters'] for m in model_comparison]\n",
    "sizes = [m['size_mb'] for m in model_comparison]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1.bar(names, params)\n",
    "ax1.set_title('Model Parameter Count')\n",
    "ax1.set_ylabel('Parameters')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "ax2.bar(names, sizes)\n",
    "ax2.set_title('Model Size (MB)')\n",
    "ax2.set_ylabel('Size (MB)')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Research Insights and Analysis\n",
    "\n",
    "Let's analyze the relationship between different components and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the relationship between text length and synthesis complexity\n",
    "test_sentences = [\n",
    "    \"Hi.\",\n",
    "    \"Hello world!\",\n",
    "    \"This is a medium length sentence for testing.\",\n",
    "    \"This is a much longer sentence that contains many more words and should result in a longer mel spectrogram output for our text-to-speech synthesis system.\"\n",
    "]\n",
    "\n",
    "analysis_results = []\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    # Process text\n",
    "    processed = text_processor.normalize_text(sentence)\n",
    "    sequence = text_processor.text_to_sequence(processed)\n",
    "    \n",
    "    # Get synthesis info\n",
    "    info = synthesizer.get_synthesis_info(sentence)\n",
    "    \n",
    "    analysis_results.append({\n",
    "        'text': sentence,\n",
    "        'char_count': len(sentence),\n",
    "        'word_count': len(sentence.split()),\n",
    "        'sequence_length': len(sequence),\n",
    "        'estimated_duration': info['estimated_duration'],\n",
    "        'estimated_mel_frames': info['estimated_mel_frames']\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "df = pd.DataFrame(analysis_results)\n",
    "print(\"Text Length Analysis:\")\n",
    "print(df)\n",
    "\n",
    "# Visualize relationships\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Character count vs sequence length\n",
    "axes[0, 0].scatter(df['char_count'], df['sequence_length'])\n",
    "axes[0, 0].set_xlabel('Character Count')\n",
    "axes[0, 0].set_ylabel('Sequence Length')\n",
    "axes[0, 0].set_title('Characters vs Sequence Length')\n",
    "\n",
    "# Word count vs estimated duration\n",
    "axes[0, 1].scatter(df['word_count'], df['estimated_duration'])\n",
    "axes[0, 1].set_xlabel('Word Count')\n",
    "axes[0, 1].set_ylabel('Estimated Duration (s)')\n",
    "axes[0, 1].set_title('Words vs Duration')\n",
    "\n",
    "# Sequence length vs mel frames\n",
    "axes[1, 0].scatter(df['sequence_length'], df['estimated_mel_frames'])\n",
    "axes[1, 0].set_xlabel('Sequence Length')\n",
    "axes[1, 0].set_ylabel('Estimated Mel Frames')\n",
    "axes[1, 0].set_title('Sequence vs Mel Frames')\n",
    "\n",
    "# Text complexity distribution\n",
    "axes[1, 1].hist(df['char_count'], alpha=0.7, label='Characters', bins=5)\n",
    "axes[1, 1].hist(df['word_count'], alpha=0.7, label='Words', bins=5)\n",
    "axes[1, 1].set_xlabel('Count')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Text Complexity Distribution')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Future Research Directions\n",
    "\n",
    "Based on our experiments, let's outline potential research directions and improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance analysis and bottleneck identification\n",
    "import time\n",
    "\n",
    "performance_metrics = {}\n",
    "\n",
    "# Text processing speed\n",
    "test_text = \"This is a performance test sentence for measuring processing speed.\"\n",
    "start_time = time.time()\n",
    "for _ in range(100):\n",
    "    sequence = text_processor.text_to_sequence(test_text)\n",
    "text_processing_time = time.time() - start_time\n",
    "performance_metrics['text_processing_per_call'] = text_processing_time / 100\n",
    "\n",
    "# Model inference speed\n",
    "model.eval()\n",
    "test_input = torch.randint(0, text_processor.vocab_size, (1, 50)).to(device)\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    for _ in range(10):\n",
    "        output = model.inference(test_input, max_len=100)\n",
    "model_inference_time = time.time() - start_time\n",
    "performance_metrics['model_inference_per_call'] = model_inference_time / 10\n",
    "\n",
    "# Audio processing speed\n",
    "start_time = time.time()\n",
    "for _ in range(10):\n",
    "    mel_spec = audio_processor.compute_mel_spectrogram(test_audio)\n",
    "audio_processing_time = time.time() - start_time\n",
    "performance_metrics['audio_processing_per_call'] = audio_processing_time / 10\n",
    "\n",
    "print(\"Performance Analysis:\")\n",
    "for metric, value in performance_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} seconds\")\n",
    "\n",
    "# Memory usage analysis\n",
    "if torch.cuda.is_available():\n",
    "    memory_allocated = torch.cuda.memory_allocated(device) / 1024**2  # MB\n",
    "    memory_reserved = torch.cuda.memory_reserved(device) / 1024**2   # MB\n",
    "    print(f\"\\nGPU Memory Usage:\")\n",
    "    print(f\"  Allocated: {memory_allocated:.2f} MB\")\n",
    "    print(f\"  Reserved: {memory_reserved:.2f} MB\")\n",
    "\n",
    "# Research recommendations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESEARCH INSIGHTS AND RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "recommendations = [\n",
    "    \"1. Model Architecture:\",\n",
    "    \"   - Experiment with transformer-based architectures (FastSpeech 2)\",\n",
    "    \"   - Investigate attention mechanisms (location-based, GMM attention)\",\n",
    "    \"   - Compare different encoder architectures (CNN vs RNN vs Transformer)\",\n",
    "    \"\",\n",
    "    \"2. Training Strategies:\",\n",
    "    \"   - Implement curriculum learning (start with short sentences)\",\n",
    "    \"   - Use teacher forcing with scheduled sampling\",\n",
    "    \"   - Apply progressive training (increase model complexity gradually)\",\n",
    "    \"\",\n",
    "    \"3. Data and Preprocessing:\",\n",
    "    \"   - Implement robust text normalization for various domains\",\n",
    "    \"   - Explore different phoneme representations (IPA, ARPAbet)\",\n",
    "    \"   - Investigate multilingual training strategies\",\n",
    "    \"\",\n",
    "    \"4. Vocoder Improvements:\",\n",
    "    \"   - Implement WaveGlow or HiFi-GAN vocoders\",\n",
    "    \"   - Explore neural vocoders with different conditioning\",\n",
    "    \"   - Compare different mel spectrogram representations\",\n",
    "    \"\",\n",
    "    \"5. Evaluation and Metrics:\",\n",
    "    \"   - Implement perceptual evaluation metrics (PESQ, STOI)\",\n",
    "    \"   - Develop speaker similarity metrics\",\n",
    "    \"   - Create automated naturalness assessment\",\n",
    "    \"\",\n",
    "    \"6. Advanced Features:\",\n",
    "    \"   - Multi-speaker synthesis with speaker embeddings\",\n",
    "    \"   - Emotion and style control\",\n",
    "    \"   - Real-time synthesis optimization\",\n",
    "    \"   - Few-shot voice cloning capabilities\"\n",
    "]\n",
    "\n",
    "for recommendation in recommendations:\n",
    "    print(recommendation)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NEXT STEPS FOR EXPERIMENTATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "next_steps = [\n",
    "    \"1. Collect and prepare a real speech dataset (LJSpeech, VCTK)\",\n",
    "    \"2. Implement proper training loop with validation and checkpointing\",\n",
    "    \"3. Experiment with different loss functions and weights\",\n",
    "    \"4. Implement attention visualization and analysis tools\",\n",
    "    \"5. Create automatic hyperparameter tuning pipeline\",\n",
    "    \"6. Develop real-time synthesis demo application\",\n",
    "    \"7. Implement model distillation for deployment optimization\",\n",
    "    \"8. Create comprehensive evaluation benchmark\"\n",
    "]\n",
    "\n",
    "for step in next_steps:\n",
    "    print(step)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has demonstrated the key components of our speech synthesis research pipeline:\n",
    "\n",
    "1. **Text Processing**: Comprehensive text normalization, phoneme conversion, and sequence encoding\n",
    "2. **Audio Processing**: Mel spectrogram computation, feature extraction, and audio analysis\n",
    "3. **Model Architecture**: Tacotron-style TTS model with encoder-decoder attention\n",
    "4. **Training Pipeline**: Dataset creation, loss computation, and training simulation\n",
    "5. **Speech Synthesis**: Text-to-audio conversion with prosody control\n",
    "6. **Evaluation**: Audio quality metrics and model performance analysis\n",
    "\n",
    "The pipeline provides a solid foundation for TTS research with modular components that can be easily modified and extended. The experiments in this notebook serve as a starting point for more advanced research in neural speech synthesis.\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- The modular design allows for easy component swapping and experimentation\n",
    "- Comprehensive evaluation metrics provide insights into model performance\n",
    "- The pipeline supports both research and practical applications\n",
    "- Performance analysis helps identify optimization opportunities\n",
    "- The framework is extensible for advanced features like multi-speaker synthesis\n",
    "\n",
    "### Files Generated:\n",
    "\n",
    "This notebook works in conjunction with the following pipeline components:\n",
    "- Models: `models/tacotron.py`, `models/vocoder.py`\n",
    "- Preprocessing: `preprocessing/text_processor.py`, `preprocessing/audio_processor.py`\n",
    "- Training: `training/trainer.py`, `training/dataset.py`\n",
    "- Synthesis: `synthesis/synthesizer.py`\n",
    "- Evaluation: `evaluation/metrics.py`\n",
    "- Utilities: `utils/config.py`, `utils/visualization.py`, `utils/audio_utils.py`\n",
    "- Web Interface: `app.py` (Streamlit application)\n",
    "\n",
    "Continue experimenting and building upon this foundation to advance the state of neural speech synthesis!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
